{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab12-1: Seq2Seq Learning & Neural Machine Translation\n",
    "---\n",
    "111062117 黃祥陞\n",
    "\n",
    "In this assignment, we are required to implement Luong Attention, where the `score` function is defined as:\n",
    "\n",
    "$$\n",
    "\\text{score}(s_t, h_i) = s_t^T W_a h_i\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $h_i$: hidden state of the encoder.\n",
    "- $s_t$: hidden state of the decoder.\n",
    "- $W_a$: the trainable weight matrix.\n",
    "\n",
    "The following sections will provide a concise explanation of each code segment's purpose and functionality.\n",
    "\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Loading\n",
    "\n",
    "Loads the IMDB movie reviews dataset for sentiment analysis from a CSV file containing reviews and sentiment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "movie_reviews = pd.read_csv(\"./data/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews[\"review\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Preprocessing\n",
    "\n",
    "- **Text Preprocessing**:\n",
    "  - Removes HTML tags.\n",
    "  - Filters out punctuation and numbers.\n",
    "  - Removes single characters and extra spaces.\n",
    "\n",
    "- **Label Encoding**: Converts sentiment labels to binary values (positive = 1, negative = 0).\n",
    "\n",
    "- **Data Splitting**: Splits the dataset into training and testing sets with an 80-20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "sentences = list(movie_reviews['review'])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))\n",
    "\n",
    "# replace the positive with 1, replace the negative with 0\n",
    "y = movie_reviews['sentiment']\n",
    "y = np.array(list(map(lambda x: 1 if x == \"positive\" else 0, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training data: 40000\n",
      "# test data: 10000\n"
     ]
    }
   ],
   "source": [
    "# Split the training dataset and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "print(\"# training data: {:d}\\n# test data: {:d}\".format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Tokenization and Padding\n",
    "\n",
    "- **Tokenization**: Initializes a tokenizer with a vocabulary size of 10,000 and fits it on the training data to convert words into integer indices.\n",
    "\n",
    "- **Text to Sequences**: Transforms each review in the training and testing sets into sequences of integers based on the tokenizer.\n",
    "\n",
    "- **Padding**: Pads all sequences to a maximum length of 100 to ensure uniform input shape for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_len = 100\n",
    "# padding sentences to the same length\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=max_len)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,  296,  140, 2854,    2,  405,  614,    1,  263,    5, 3514,\n",
       "        977,    4,   25,   37,   11, 1237,  215,   62,    2,   35,    5,\n",
       "         27,  217,   24,  189, 1430,    7, 1068,   15, 4868,   81,    1,\n",
       "        221,   63,  351,   64,   52,   24,    4, 3547,   13,    6,   19,\n",
       "        192,    4, 8148,  859, 3430, 1720,   17,   23,    4,  158,  194,\n",
       "        175,  106,    9, 1604,  461,   71,  218,    4,  321,    2, 3431,\n",
       "         31,   20,   47,   68, 1844, 4668,   11,    6, 1365,    8,   16,\n",
       "          5, 3475, 1990,   14,   59,    1, 2380,  460,  518,    2,  170,\n",
       "       2524, 2698, 1745,    4,  573,    6,   33,    1, 3750,  198,  345,\n",
       "       3812], dtype=int32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the preprocessed data\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "\n",
    "- **Batch and Buffer Size**: Sets buffer and batch sizes for efficient data shuffling and batching.\n",
    "- **Embedding and Units Configuration**: Defines vocabulary size, embedding dimension, and GRU units for the encoder model.\n",
    "- **Dataset Creation**: Creates TensorFlow datasets for training and testing with shuffling and batching applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([128, 100]), TensorShape([128]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(X_train)\n",
    "BATCH_SIZE = 128\n",
    "steps_per_epoch = len(X_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "# only reserve 10000 words\n",
    "vocab_size = 10000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Encoder Model Initialization\n",
    "\n",
    "**Encoder Model**: Initializes an encoder with an embedding layer and a GRU layer. The encoder transforms tokenized sequences into hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        # vacab_size=10000, embedding_dim=256 enc_units=1024 batch_sz=64\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_activation='sigmoid',\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        # x is the training data with shape == (batch_size，max_length)  -> (128, 100)\n",
    "        # which means there are batch_size sentences in one batch, the length of each sentence is max_length\n",
    "        # hidden state shape == (batch_size, units) -> (128, 1024)\n",
    "        # after embedding, x shape == (batch_size, max_length, embedding_dim) -> (128, 100, 256)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # output contains the state(in GRU, the hidden state and the output are same) from all timestamps,\n",
    "        # output shape == (batch_size, max_length, units) -> (128, 100, 1024)\n",
    "        # state is the hidden state of the last timestamp, shape == (batch_size, units) -> (128, 1024)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        \n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        # initialize the first state of the gru,  shape == (batch_size, units) -> (128, 1024)\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (128, 100, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (128, 1024)\n",
      "tf.Tensor([ True  True  True ...  True  True  True], shape=(1024,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
    "# the output and the hidden state of GRU is equal\n",
    "print(sample_output[-1, -1, :] == sample_hidden[-1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Luong Attention Mechanism\n",
    "\n",
    "Implements `Luong Attention`, which calculates attention scores by computing the dot product between the encoder outputs (values) and the decoder's hidden state (query). The attention mechanism is defined as:\n",
    "\n",
    "$$\n",
    "\\text{score}(s_t, h_i) = s_t^T W_a h_i\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $s_t$ is the current hidden state of the decoder (query),\n",
    "- $h_i$ is the hidden state of the encoder at each timestep (values),\n",
    "- $W_a$ is a trainable weight matrix.\n",
    "\n",
    "### Steps\n",
    "\n",
    "- **Weight Matrix**: Initializes a dense layer $W_a$ to transform encoder outputs before calculating the attention scores.\n",
    "  \n",
    "- **Score Calculation**: Applies $W_a$ to the encoder outputs, then calculates the dot product between the transformed outputs and the decoder's hidden state. This results in a score for each encoder hidden state, representing its relevance to the current decoder state.\n",
    "\n",
    "  $$\n",
    "  \\text{score} = \\text{values} \\cdot s_t^T\n",
    "  $$\n",
    "\n",
    "- **Attention Weights**: Uses $\\text{softmax}$ on the scores to produce attention weights $\\alpha_i$, indicating the importance of each encoder hidden state. i.e., $\\alpha_i=\\text{softmax(score)}$.\n",
    "- **Context Vector**: Calculates a weighted sum of encoder outputs using the attention weights, producing a context vector. This context vector helps the decoder focus on relevant parts of the input sequence.\n",
    "\n",
    "  $$\n",
    "  \\text{context vector} = \\sum_i \\alpha_i h_i\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.W_a = tf.keras.layers.Dense(units)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query shape: (batch_size, hidden_size)\n",
    "        # values shape: (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # Apply W_a to values (encoder outputs), shape becomes (batch_size, max_length, units)\n",
    "        values = self.W_a(values)\n",
    "        \n",
    "        # Compute the score by taking dot product between query and transformed values\n",
    "        # score shape: (batch_size, max_length)\n",
    "        score = tf.matmul(values, query[:, tf.newaxis], transpose_b=True)\n",
    "        score = tf.squeeze(score, axis=-1)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # Compute context vector as the weighted sum of values\n",
    "        context_vector = tf.reduce_sum(values * attention_weights[:, :, tf.newaxis], axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Decoder Model\n",
    "\n",
    "The `Decoder` model uses `Luong Attention` to calculate a context vector from encoder outputs and passes it through fully connected layers to predict the sentiment score.\n",
    "\n",
    "- **Attention**: `LuongAttention` computes the context vector based on encoder outputs and the current decoder state.\n",
    "- **Fully Connected Layers**: The context vector is processed through four dense layers, with the final layer outputting a sentiment score (positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        \n",
    "        # pass through four fully connected layers, the model will return \n",
    "        # the probability of the positivity of the sentence\n",
    "        self.fc_1 = tf.keras.layers.Dense(2048)\n",
    "        self.fc_2 = tf.keras.layers.Dense(512)\n",
    "        self.fc_3 = tf.keras.layers.Dense(64)\n",
    "        self.fc_4 = tf.keras.layers.Dense(1)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = LuongAttention(self.dec_units)\n",
    "\n",
    "    def call(self, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        output = self.fc_1(context_vector)\n",
    "        output = self.fc_2(output)\n",
    "        output = self.fc_3(output)\n",
    "        output = self.fc_4(output)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (128, 1)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(units, BATCH_SIZE)\n",
    "sample_decoder_output, _ = decoder(sample_hidden, sample_output)\n",
    "print('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Training and Optimization\n",
    "\n",
    "This section sets up the optimizer, loss function, and training loop for the sentiment analysis model.\n",
    "\n",
    "- **Optimizer**: Uses `Adam` optimizer to update model weights.\n",
    "  \n",
    "- **Loss Function**: Defines binary cross-entropy as the loss function for binary sentiment classification. The `loss_function` computes the mean loss for each batch.\n",
    "\n",
    "- **Checkpointing**: Sets up a checkpoint system to save model weights every 2 epochs, ensuring training progress can be saved and restored.\n",
    "\n",
    "- **Training Loop**:\n",
    "  - For each epoch, initializes the encoder's hidden state and iterates over batches in the dataset.\n",
    "  - Calls `train_step` to compute the forward pass, calculate gradients, and apply weight updates.\n",
    "  - Logs batch loss every 100 batches and epoch loss at the end of each epoch.\n",
    "  - Measures and prints the time taken for each epoch to monitor training efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam()\n",
    "optimizer = tf.keras.optimizers.legacy.Adam() # for M1/M2 Macs\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_ = loss_object(real, pred)\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './checkpoints/sentiment-analysis'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        # passing enc_output to the decoder\n",
    "        predictions, _ = decoder(enc_hidden, enc_output)\n",
    "\n",
    "        loss = loss_function(targ, predictions)\n",
    "\n",
    "    # collect all trainable variables\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    # calculate the gradients for the whole variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    # apply the gradients on the variables\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.6932\n",
      "Epoch 1 Batch 100 Loss 0.3759\n",
      "Epoch 1 Batch 200 Loss 0.2599\n",
      "Epoch 1 Batch 300 Loss 0.3304\n",
      "Epoch 1 Loss 0.3836\n",
      "Time taken for 1 epoch 472.62368392944336 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.2367\n",
      "Epoch 2 Batch 100 Loss 0.1418\n",
      "Epoch 2 Batch 200 Loss 0.3096\n",
      "Epoch 2 Batch 300 Loss 0.2727\n",
      "Epoch 2 Loss 0.2566\n",
      "Time taken for 1 epoch 524.7104339599609 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.1585\n",
      "Epoch 3 Batch 100 Loss 0.1784\n",
      "Epoch 3 Batch 200 Loss 0.2071\n",
      "Epoch 3 Batch 300 Loss 0.2507\n",
      "Epoch 3 Loss 0.1909\n",
      "Time taken for 1 epoch 540.1827671527863 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1043\n",
      "Epoch 4 Batch 100 Loss 0.1103\n",
      "Epoch 4 Batch 200 Loss 0.1371\n",
      "Epoch 4 Batch 300 Loss 0.2004\n",
      "Epoch 4 Loss 0.1256\n",
      "Time taken for 1 epoch 566.4552917480469 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0764\n",
      "Epoch 5 Batch 100 Loss 0.1516\n",
      "Epoch 5 Batch 200 Loss 0.0625\n",
      "Epoch 5 Batch 300 Loss 0.1146\n",
      "Epoch 5 Loss 0.0928\n",
      "Time taken for 1 epoch 567.2183699607849 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0447\n",
      "Epoch 6 Batch 100 Loss 0.0218\n",
      "Epoch 6 Batch 200 Loss 0.0612\n",
      "Epoch 6 Batch 300 Loss 0.0261\n",
      "Epoch 6 Loss 0.0568\n",
      "Time taken for 1 epoch 590.6184477806091 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0139\n",
      "Epoch 7 Batch 100 Loss 0.0310\n",
      "Epoch 7 Batch 200 Loss 0.0098\n",
      "Epoch 7 Batch 300 Loss 0.0325\n",
      "Epoch 7 Loss 0.0392\n",
      "Time taken for 1 epoch 593.0800399780273 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0123\n",
      "Epoch 8 Batch 100 Loss 0.0041\n",
      "Epoch 8 Batch 200 Loss 0.0232\n",
      "Epoch 8 Batch 300 Loss 0.0320\n",
      "Epoch 8 Loss 0.0324\n",
      "Time taken for 1 epoch 587.3291878700256 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0263\n",
      "Epoch 9 Batch 100 Loss 0.0164\n",
      "Epoch 9 Batch 200 Loss 0.0354\n",
      "Epoch 9 Batch 300 Loss 0.0125\n",
      "Epoch 9 Loss 0.0326\n",
      "Time taken for 1 epoch 606.4436910152435 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0195\n",
      "Epoch 10 Batch 100 Loss 0.0330\n",
      "Epoch 10 Batch 200 Loss 0.1260\n",
      "Epoch 10 Batch 300 Loss 0.0199\n",
      "Epoch 10 Loss 0.0259\n",
      "Time taken for 1 epoch 655.3101110458374 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set the epochs for training\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # get the initial hidden state of gru\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "# # restoring the latest checkpoint in checkpoint_dir\n",
    "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(inp, enc_hidden):\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        predictions, attention_weights = decoder(enc_hidden, enc_output)\n",
    "    return predictions, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_data):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    \n",
    "    for batch, (inp, targ) in enumerate(test_data):\n",
    "        if len(inp) != BATCH_SIZE:\n",
    "            enc_hidden = tf.zeros((len(inp), units))\n",
    "        # make prediction\n",
    "        if batch == 0:\n",
    "            predictions, attention_weights = test_step(inp, enc_hidden)\n",
    "            predictions, attention_weights = predictions.numpy(), attention_weights.numpy()\n",
    "        else:\n",
    "            _predictions, _attention_weights = test_step(inp, enc_hidden)\n",
    "            _predictions, _attention_weights = _predictions.numpy(), _attention_weights.numpy()\n",
    "            predictions = np.concatenate((predictions, _predictions))\n",
    "            attention_weights = np.concatenate((attention_weights, _attention_weights))\n",
    "    \n",
    "    predictions = np.squeeze(predictions)\n",
    "    attention_weights = np.squeeze(attention_weights)\n",
    "    predictions[np.where(predictions < 0.5)] = 0\n",
    "    predictions[np.where(predictions >= 0.5)] = 1\n",
    "    return predictions, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation and Attention Visualization\n",
    "\n",
    "Evaluates the model on the test dataset and visualizes attention weights for sample predictions.\n",
    "\n",
    "- **Evaluation**:\n",
    "  - Calls `evaluate` to get predictions and attention weights on the test dataset.\n",
    "  - Calculates the model’s accuracy by comparing predictions with true labels.\n",
    "\n",
    "- **Attention Visualization**:\n",
    "  - Defines a `print_colored` function to color-highlight words based on attention weights.\n",
    "  - For each of the first 10 test samples:\n",
    "    - Displays the true label and the predicted label.\n",
    "    - Highlights the top 10 most attended words in red to visualize which parts of the text the model focused on when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, attention_weights = evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8492\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', (y_pred == y_test).sum() / len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true: 1\n",
      "y_predict: 0\n",
      "changed it was terrible main event just like every \u001b[31mmatch\u001b[0m is in is terrible other matches on the card were razor ramon vs ted brothers vs bodies shawn michaels \u001b[31mvs\u001b[0m this was the event where shawn named his big monster \u001b[31mof\u001b[0m body guard vs kid \u001b[31mhart\u001b[0m \u001b[31mfirst\u001b[0m takes on then takes on jerry and stuff with the and was always very interesting then destroyed marty undertaker took \u001b[31mon\u001b[0m giant in another terrible \u001b[31mmatch\u001b[0m the smoking and took \u001b[31mon\u001b[0m bam bam and the and the world title \u001b[31magainst\u001b[0m lex this \u001b[31mmatch\u001b[0m was boring and it has terrible ending however it deserves \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "of subject matter as are and broken in many ways on many many \u001b[31missues\u001b[0m happened to see the pilot \u001b[31mpremiere\u001b[0m \u001b[31min\u001b[0m \u001b[31mpassing\u001b[0m and just had to keep in after that to see if would ever get the girl after seeing them all on television was delighted to see them available on dvd have to admit that it was the \u001b[31monly\u001b[0m thing that kept me sane whilst had to do hour night \u001b[31mshift\u001b[0m \u001b[31mand\u001b[0m developed insomnia farscape was the \u001b[31monly\u001b[0m thing to get me through \u001b[31mthose\u001b[0m extremely \u001b[31mlong\u001b[0m nights do yourself favour watch the pilot and see what mean farscape comet \n",
      "\n",
      "\n",
      "y_true: 0\n",
      "y_predict: 0\n",
      "destruction the first really bad thing is the guy \u001b[31msteven\u001b[0m \u001b[31mseagal\u001b[0m \u001b[31mwould\u001b[0m have been beaten to \u001b[31mpulp\u001b[0m by \u001b[31mseagal\u001b[0m driving but that \u001b[31mprobably\u001b[0m \u001b[31mwould\u001b[0m have ended the whole premise for the movie it seems like they decided to make all kinds of changes in the movie plot so just plan to enjoy the action and do not expect coherent plot turn any sense of logic you \u001b[31mmay\u001b[0m have it will your chance of getting headache does give me some hope that steven \u001b[31mseagal\u001b[0m is \u001b[31mtrying\u001b[0m to move back towards the type of characters he portrayed in his more popular movies \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "jane austen would definitely of this one paltrow does an \u001b[31mawesome\u001b[0m job capturing \u001b[31mthe\u001b[0m attitude of emma she is funny without being silly yet elegant she puts on very convincing british accent not being british myself maybe m not the best judge but she fooled me she was also excellent in doors sometimes forget \u001b[31mshe\u001b[0m american \u001b[31malso\u001b[0m brilliant are \u001b[31mjeremy\u001b[0m \u001b[31mnortham\u001b[0m \u001b[31mand\u001b[0m \u001b[31msophie\u001b[0m \u001b[31mthompson\u001b[0m and law emma thompson sister and mother as the bates women they \u001b[31mnearly\u001b[0m steal the show and ms law doesn even have any lines highly recommended \n",
      "\n",
      "\n",
      "y_true: 0\n",
      "y_predict: 0\n",
      "reaches the point where they become \u001b[31mobnoxious\u001b[0m and simply \u001b[31mfrustrating\u001b[0m touch football puzzle family and talent shows are not how actual people behave it almost sickening another big flaw is the woman carell is supposed to be falling for her in her first scene with steve carell is like watching \u001b[31mstroke\u001b[0m victim trying to be \u001b[31mwhat\u001b[0m \u001b[31mimagine\u001b[0m is \u001b[31msupposed\u001b[0m to be unique and original in this woman \u001b[31mcomes\u001b[0m off as mildly retarded it makes me think that this movie is taking place on another planet left the theater wondering \u001b[31mwhat\u001b[0m \u001b[31mjust\u001b[0m saw after thinking \u001b[31mfurther\u001b[0m don think it was much \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "the pace quick and \u001b[31menergetic\u001b[0m but \u001b[31mmost\u001b[0m importantly he knows how to make comedy funny he doesn the jokes and he \u001b[31munderstands\u001b[0m that funny actors know what they re doing and he allows them to do it but segal goes step further he gives tommy boy friendly almost nostalgic \u001b[31mtone\u001b[0m that both the genuinely and the critics \u001b[31mdidn\u001b[0m \u001b[31mlike\u001b[0m tommy \u001b[31mboy\u001b[0m shame on them movie doesn have to be super sophisticated or \u001b[31mintellectual\u001b[0m to be funny god \u001b[31mfarley\u001b[0m \u001b[31mand\u001b[0m spade were forced to do muted comedy la the office this is great movie and one of my all time favorites \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "for once story of hope over the tragic reality our youth face rising draws one into scary and \u001b[31munfair\u001b[0m world and shows through \u001b[31mbeautiful\u001b[0m \u001b[31mcolor\u001b[0m and moving music how one man and his dedicated friends choose not to accept that world and change it through action and art an entertaining interesting \u001b[31memotional\u001b[0m beautiful film showed this film to numerous high school students as well who all \u001b[31mlive\u001b[0m in with poverty and and gun violence and they were with \u001b[31manderson\u001b[0m \u001b[31mthe\u001b[0m protagonist recommend this film to all ages over due to \u001b[31msubtitles\u001b[0m and \u001b[31msome\u001b[0m images of death from all \u001b[31mbackgrounds\u001b[0m \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "people and sleeping around that he kept \u001b[31msecret\u001b[0m from most people he feels free to have an affair with \u001b[31mquasi\u001b[0m \u001b[31mbecause\u001b[0m \u001b[31mhe\u001b[0m kevin \u001b[31mhe\u001b[0m figures \u001b[31mout\u001b[0m \u001b[31mthat\u001b[0m he can \u001b[31mfool\u001b[0m some people with \u001b[31mcards\u001b[0m like hotel but it won get him out of those the of heaven \u001b[31mare\u001b[0m keeping track of him and everything he does after reading all the theories on though it seems like identity is reminder of the different paths tony could ve taken in his life possibly along with the car joke involving that made no sense to me otherwise at that point my brain out \n",
      "\n",
      "\n",
      "y_true: 0\n",
      "y_predict: 0\n",
      "over again can remember how many times he said the universe is made out of tiny little strings it like they were trying to us into just accepting are the best thing since bread \u001b[31mfinally\u001b[0m the show ended off with an unpleasant sense of competition between and clearly biased towards this is supposed to be an educational program about quantum physics not about \u001b[31mwhether\u001b[0m \u001b[31mthe\u001b[0m \u001b[31mus\u001b[0m is better than \u001b[31meurope\u001b[0m or vice \u001b[31mversa\u001b[0m also felt that was part of the audiences \u001b[31mneed\u001b[0m to see some conflict to remain \u001b[31minterested\u001b[0m please give me little \u001b[31mmore\u001b[0m credit \u001b[31mthan\u001b[0m that overall thumbs down \n",
      "\n",
      "\n",
      "y_true: 0\n",
      "y_predict: 0\n",
      "the scenes involving joe character in particular the scenes in the terribly clich but still funny rich but screwed up \u001b[31mcharacters\u001b[0m house where the story towards \u001b[31mit\u001b[0m final moments can see how was great stage play and while the film makers did their best to translate this to celluloid it simply \u001b[31mdidn\u001b[0m \u001b[31mwork\u001b[0m and while \u001b[31mlaughed\u001b[0m \u001b[31mout\u001b[0m \u001b[31mloud\u001b[0m at \u001b[31msome\u001b[0m of scenes and one liners think the first minutes my senses and expectations to such degree would have laughed at anything \u001b[31munless\u001b[0m you re stuck \u001b[31mfor\u001b[0m novelty coffee coaster don pick this up if you see it in bargain bucket \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_colored(text, color_code):\n",
    "    return f'{color_code}{text}\\033[0m'\n",
    "\n",
    "for idx, data in enumerate(X_test[:10]):\n",
    "    print('y_true: {:d}'.format(y_test[idx]))\n",
    "    print('y_predict: {:.0f}'.format(y_pred[idx]))\n",
    "    \n",
    "    large_weights_idx = np.argsort(attention_weights[idx])[::-1][:10]\n",
    "    \n",
    "    for _idx in range(len(data)):\n",
    "        word_idx = data[_idx]\n",
    "        if word_idx != 0:\n",
    "            if _idx in large_weights_idx:\n",
    "                print(print_colored(tokenizer.index_word[word_idx], '\\033[31m'), end=' ')\n",
    "            else:\n",
    "                print(tokenizer.index_word[word_idx], end=' ')\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
