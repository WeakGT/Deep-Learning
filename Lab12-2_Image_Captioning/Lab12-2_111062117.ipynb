{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab12-2: Image Caption\n",
    "---\n",
    "111062117 黃祥陞\n",
    "\n",
    "In this assignment, we built a CAPTCHA recognition model using CNNs and RNNs with attention. CNNs extract image features, and RNNs generate sequences, focusing on relevant parts with attention. This approach effectively recognizes and translates CAPTCHA images into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "\n",
    "# with zipfile.ZipFile('words_captcha.zip') as zip_ref:\n",
    "#     zip_ref.extractall('words_captcha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 140000 images and 120000 labels.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Load image paths and captions\n",
    "img_name_list = []\n",
    "cap_list = []\n",
    "\n",
    "with open('./words_captcha/spec_train_val.txt') as file:\n",
    "    for line in file:\n",
    "        image_name, caption = line.strip().split()\n",
    "        img_name_list.append(f'./words_captcha/{image_name}.png')\n",
    "        cap_list.append('<start> ' + ' '.join(caption) + ' <end>')\n",
    "\n",
    "test_img_name_list = set(glob.glob('./words_captcha/*.png')) - set(img_name_list)\n",
    "img_name_list += list(sorted(test_img_name_list))\n",
    "\n",
    "print(f\"Loaded {len(img_name_list)} images and {len(cap_list)} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "This section tokenizes captions, converts them into integer sequences, and pads them to uniform length. It ensures captions are properly formatted for model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> t h u s <end>\n",
      "[ 2  9 18 17  6  3  0]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize captions\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', oov_token='')\n",
    "tokenizer.fit_on_texts(cap_list)\n",
    "\n",
    "# Convert captions to sequences and pad them\n",
    "cap_seqs = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(cap_list), padding='post')\n",
    "max_length = cap_seqs.shape[1]\n",
    "\n",
    "# Verify results\n",
    "print(cap_list[0])  # Example caption in text\n",
    "print(cap_seqs[0])  # Example caption as indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 100000\n",
      "Validation set size: 20000\n",
      "Test set size: 20000\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_image_paths, val_image_paths, test_image_paths = img_name_list[:100000], img_name_list[100000:120000], img_name_list[120000:]\n",
    "train_captions, val_captions = cap_seqs[:100000], cap_seqs[100000:]\n",
    "\n",
    "# Verify the lengths of the splits\n",
    "print(f\"Training set size: {len(train_image_paths)}\")\n",
    "print(f\"Validation set size: {len(val_image_paths)}\")\n",
    "print(f\"Test set size: {len(test_image_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "IMAGE_SIZE = (160, 300)\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000\n",
    "EMBEDDING_DIM = 256\n",
    "UNITS = 512\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "NUM_STEPS = len(train_image_paths) // BATCH_SIZE\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset\n",
    "\n",
    "This section defines a function to preprocess images and pairs them with their corresponding captions. TensorFlow `Dataset` objects are created for training and validation data, enabling efficient batching, shuffling, and prefetching to optimize data loading during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)\n",
    "    img = tf.keras.applications.densenet.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = load_image(img_name)\n",
    "    return img_tensor, cap\n",
    "\n",
    "# Create TensorFlow Dataset objects\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_image_paths, train_captions))\\\n",
    "                               .map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                               .shuffle(BUFFER_SIZE)\\\n",
    "                               .batch(BATCH_SIZE)\\\n",
    "                               .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_image_paths, val_captions))\\\n",
    "                             .map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                             .batch(BATCH_SIZE)\\\n",
    "                             .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Verify the dataset\n",
    "# for img, cap in train_dataset.take(1):\n",
    "#     print(img.shape)\n",
    "#     print(cap.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "\n",
    "### Feature Extractor\n",
    "The DenseNet121 model, pre-trained on ImageNet, is used as the feature extractor. Its top layers are removed, allowing the output feature maps to serve as input for the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import DenseNet121\n",
    "# Load the DenseNet121 model pre-trained on ImageNet\n",
    "feature_extractor = DenseNet121(include_top=False, weights='imagenet', input_shape=(*IMAGE_SIZE, 3))\n",
    "\n",
    "# feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Encoder\n",
    "A CNN-based encoder is defined to process the feature maps. It reduces the dimensionality using a fully connected layer followed by ReLU activation, preparing the data for the attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bahdanau attention mechanism allows the model to focus on specific parts of the input features dynamically while generating each word in the output sequence. It takes the encoder’s output feature maps and the decoder’s hidden state as inputs. The hidden state is expanded to match the dimensions of the feature maps, and both are passed through separate dense layers. A combined score is computed using a `tanh` activation, followed by a softmax function to generate attention weights. These weights indicate the importance of each feature and are used to calculate a weighted sum of the feature maps, known as the context vector. The context vector guides the decoder in predicting the next word, enabling the model to adaptively attend to different parts of the input for improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An RNN decoder is constructed to generate captions. It combines an embedding layer, a GRU, and fully connected layers. The decoder integrates context vectors from the attention mechanism to sequentially predict words in the caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(EMBEDDING_DIM)\n",
    "decoder = RNN_Decoder(EMBEDDING_DIM, UNITS, VOCAB_SIZE)\n",
    "# optizmizer = tf.keras.optimizers.Adam()\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "This section sets up the training process, including checkpointing, a training step function, and the main training loop. Checkpoints are managed to save and restore model weights, optimizer states, and training progress. The `train_step` function computes the loss and updates the model weights using the feature extractor, encoder, and decoder. The training loop iterates through the dataset for the specified epochs, logging the loss at each step and saving the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/densenet\"\n",
    "ckpt = tf.train.Checkpoint(feature_extractor=feature_extractor,\n",
    "                           encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from ./checkpoints/densenet/ckpt-3\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint).expect_partial()\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    print(f\"Restored from {ckpt_manager.latest_checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = feature_extractor(img_tensor, True)\n",
    "        features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
    "        features = encoder(features)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables + feature_extractor.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_plot = []\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(train_dataset, total=NUM_STEPS, desc=f'Epoch {epoch + 1:2d}')\n",
    "    for (step, (img_tensor, target)) in enumerate(pbar):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        pbar.set_postfix({'loss': total_loss.numpy() / (step + 1)})\n",
    "\n",
    "    # Save the model every epochs\n",
    "    ckpt_manager.save()\n",
    "\n",
    "    epoch_loss = total_loss / NUM_STEPS\n",
    "    loss_plot.append(epoch_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "This section defines functions for evaluation and accuracy calculation. The `process` function converts tokenized captions back into text by removing `<start>` and `<end>` markers. The `evaluate` function generates predicted captions for a batch of image tensors by passing them through the feature extractor, encoder, and decoder, dynamically predicting each word using the attention mechanism. Finally, the `calculate_accuracy` function compares the generated captions with the ground truth captions from the validation dataset, computing the accuracy by iterating through all validation samples and checking for exact matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(caption):\n",
    "    cap = ''\n",
    "    for i in caption:\n",
    "        if tokenizer.index_word[i] == '<start>':\n",
    "            continue\n",
    "        elif tokenizer.index_word[i] == '<end>':\n",
    "            break\n",
    "        cap += tokenizer.index_word[i]\n",
    "    return cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate(img_tensor):\n",
    "    batch_size = img_tensor.shape[0]\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "    \n",
    "    features = feature_extractor(img_tensor)\n",
    "    features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
    "    features = encoder(features)\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=batch_size)\n",
    "\n",
    "    result = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "    for _ in range(max_length):\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        predicted_id = tf.argmax(predictions, axis=1).numpy()\n",
    "        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "        result = tf.concat([result, predicted_id.reshape((batch_size, 1))], axis=1)\n",
    "    return result.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy():\n",
    "    pbar = tqdm(val_dataset, desc='Calculating Accuracy')\n",
    "    correct = 0\n",
    "    for img_tensor, cap in pbar:\n",
    "        pred = evaluate(img_tensor)\n",
    "        for pred, cap in zip(pred, cap.numpy()):\n",
    "            if process(pred) == process(cap):\n",
    "                correct += 1\n",
    "        pbar.set_postfix({'accuracy': correct / ((pbar.n + 1) * BATCH_SIZE)})\n",
    "    return correct / len(val_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Accuracy: 100%|██████████| 625/625 [10:58<00:00,  1.05s/it, accuracy=0.91] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurcacy: 0.9097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Accurcacy: {calculate_accuracy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the loss\n",
    "# plt.plot(loss_plot)\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Loss Plot')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Images: 100%|██████████| 625/625 [12:20<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "def map_func_test(img_path):\n",
    "    img_tensor = load_image(img_path)\n",
    "    return img_tensor, img_path\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_image_paths)\\\n",
    "                              .map(map_func_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                              .batch(BATCH_SIZE)\\\n",
    "                              .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "with open(\"Lab12-2_111062117.txt\", \"w\") as file:\n",
    "    for img_tensor, img_path in tqdm(test_dataset, desc=\"Processing Test Images\"):\n",
    "        result = evaluate(img_tensor)\n",
    "        for pred, path in zip(result, img_path):\n",
    "            cap = process(pred)\n",
    "            path = path.numpy().decode('utf-8')\n",
    "            file.write(f\"{path[-11:-4]} {cap}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used DenseNet as the feature extractor for the CAPTCHA recognition model. DenseNet’s dense connections enable efficient feature reuse, making it highly effective for capturing image details with fewer parameters. The encoder, attention mechanism, and decoder components followed the structure provided by the TA’s code.\n",
    "\n",
    "The model was trained for 3 epochs, achieving a final accuracy of 0.9097 on validation set. This demonstrates the capability of DenseNet to extract meaningful features, combined with attention and RNNs to generate accurate text from CAPTCHA images. The architecture effectively integrates these components to solve a vision-to-text task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
